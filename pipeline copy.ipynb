{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ensembled_mobil_listrik.csv')  # Replace with your file path\n",
    "df = df.dropna()\n",
    "X = df['text_cleaning']\n",
    "map_sentiment = {'positif': 2, 'netral': 1, 'negatif': 0}\n",
    "y = df['final_ensemble_sentiment'].map(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_komentar</th>\n",
       "      <th>nama_akun</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>text_cleaning</th>\n",
       "      <th>sentimen</th>\n",
       "      <th>mapped_text</th>\n",
       "      <th>sentimen_RoBERTa</th>\n",
       "      <th>sentimen_DistilBERT</th>\n",
       "      <th>sentimen_BERT</th>\n",
       "      <th>sentimen_INDOBERTweet</th>\n",
       "      <th>final_ensemble_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgwqJqu6JMF4EH2CsVV4AaABAg</td>\n",
       "      <td>Fatih Al-Ayyubi</td>\n",
       "      <td>2023-08-04 10:17:57+00:00</td>\n",
       "      <td>baik kualitas kembang dulu baik kualitas motor...</td>\n",
       "      <td>positif</td>\n",
       "      <td>baik kualitas kembang dulu baik kualitas motor...</td>\n",
       "      <td>positif</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ugx-zVY4ktd7JNUB6xV4AaABAg</td>\n",
       "      <td>Syarif Airlangga</td>\n",
       "      <td>2023-08-04 06:58:17+00:00</td>\n",
       "      <td>harga motor mahal masa harga mirip motor beat ...</td>\n",
       "      <td>positif</td>\n",
       "      <td>harga motor mahal masa harga mirip motor beat ...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ugyy3luBOOHQspWyBiR4AaABAg</td>\n",
       "      <td>Putut Parwoto</td>\n",
       "      <td>2023-08-04 01:04:18+00:00</td>\n",
       "      <td>proses kenal produk baru butuh waktu ganti ken...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>proses kenal produk baru butuh waktu ganti ken...</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UgzZY-ZoTvfiVoOv8Dt4AaABAg</td>\n",
       "      <td>gema</td>\n",
       "      <td>2023-08-03 04:23:25+00:00</td>\n",
       "      <td>kampung   sekarang banyak banget bocil sama ce...</td>\n",
       "      <td>positif</td>\n",
       "      <td>kampung sekarang banyak banget bocil sama cewe...</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UgwzritHDvD9naYvia54AaABAg</td>\n",
       "      <td>Khoirudin 22</td>\n",
       "      <td>2023-07-29 11:08:07+00:00</td>\n",
       "      <td>harga terlalu mahal</td>\n",
       "      <td>positif</td>\n",
       "      <td>harga terlalu mahal</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UgxGrdA8dD3FNULB4Rd4AaABAg</td>\n",
       "      <td>Lukman Effendi</td>\n",
       "      <td>2023-07-28 10:23:53+00:00</td>\n",
       "      <td>bapak luhut panjaitan yth baik pakai dulu dina...</td>\n",
       "      <td>positif</td>\n",
       "      <td>bapak luhut panjaitan yth baik pakai dulu dina...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ugw_osI9is5z9jHD9454AaABAg</td>\n",
       "      <td>Ricky Thunger</td>\n",
       "      <td>2023-07-27 09:20:51+00:00</td>\n",
       "      <td>subsidi motor   jalan umum tolak system</td>\n",
       "      <td>negatif</td>\n",
       "      <td>subsidi motor jalan umum tolak sistem</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ugym8DQyKPAzJntZCeZ4AaABAg</td>\n",
       "      <td>Deddy jagad semesta</td>\n",
       "      <td>2023-07-20 07:40:34+00:00</td>\n",
       "      <td>tambah bikin rakyat susah</td>\n",
       "      <td>netral</td>\n",
       "      <td>tambah bikin rakyat susah</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>UgxFMeOytVzzJo6iy5F4AaABAg</td>\n",
       "      <td>TopTrainers</td>\n",
       "      <td>2023-07-16 06:30:59+00:00</td>\n",
       "      <td>jual ev naik jual genset bakal naik ni</td>\n",
       "      <td>positif</td>\n",
       "      <td>jual ev naik jual genset bakal naik ni</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>UgxcvX9LMnprk35PR3d4AaABAg</td>\n",
       "      <td>parkir anovo</td>\n",
       "      <td>2023-07-13 05:17:26+00:00</td>\n",
       "      <td>contohin dulu semua menteri dpr mpr guna listri</td>\n",
       "      <td>negatif</td>\n",
       "      <td>contohkan dulu semua menteri dpr mpr guna listrik</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Ugz6oZ8eFg8_9QCkfHx4AaABAg</td>\n",
       "      <td>Ary Nawan</td>\n",
       "      <td>2023-07-12 06:29:27+00:00</td>\n",
       "      <td>harga sperpartnya</td>\n",
       "      <td>positif</td>\n",
       "      <td>harga suku cadangnya</td>\n",
       "      <td>netral</td>\n",
       "      <td>positif</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ugw4c0eMIKC1XcKtYa14AaABAg</td>\n",
       "      <td>Zar</td>\n",
       "      <td>2023-07-11 15:18:15+00:00</td>\n",
       "      <td>ev msih nyaman dsni   dipaksain paling cman ja...</td>\n",
       "      <td>positif</td>\n",
       "      <td>ev masih nyaman disini dipaksakan paling cuman...</td>\n",
       "      <td>positif</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Ugxnals4WC_p0IuGVsB4AaABAg</td>\n",
       "      <td>agung nugroho</td>\n",
       "      <td>2023-07-11 14:14:22+00:00</td>\n",
       "      <td>nungguin ganti lithium solid state battery ril...</td>\n",
       "      <td>positif</td>\n",
       "      <td>menunggu ganti lithium solid negara baterai ri...</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>UgwAD3ZeI8HU7tJghSZ4AaABAg</td>\n",
       "      <td>RandomWM</td>\n",
       "      <td>2023-07-10 13:16:11+00:00</td>\n",
       "      <td>suat   baru kata   jaman dlu kala efisien ting...</td>\n",
       "      <td>positif</td>\n",
       "      <td>suat baru kata jaman dulu kala efisien tinggal...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>UgwpTFEpxjsXFSKYaV14AaABAg</td>\n",
       "      <td>yudhi mypras</td>\n",
       "      <td>2023-07-09 22:51:21+00:00</td>\n",
       "      <td>masyarakat ingin harga jual batre   murah ba...</td>\n",
       "      <td>positif</td>\n",
       "      <td>masyarakat ingin harga jual baterai murah baru...</td>\n",
       "      <td>positif</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Ugwp4q-M41KTZ4GawjF4AaABAg</td>\n",
       "      <td>Inta</td>\n",
       "      <td>2023-07-07 03:56:45+00:00</td>\n",
       "      <td>coba cek jabat staf ngomong soal sehariya pakai</td>\n",
       "      <td>negatif</td>\n",
       "      <td>coba cek jabat staf ngomong soal sehari-hari p...</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Ugx8_EzV1yvHhp0hK-V4AaABAg</td>\n",
       "      <td>Abner Moses</td>\n",
       "      <td>2023-07-06 11:38:46+00:00</td>\n",
       "      <td>lama bensin   laku harga lebih mahal</td>\n",
       "      <td>positif</td>\n",
       "      <td>lama bensin laku harga lebih mahal</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>UgwLp2Ai329bzdpRV2x4AaABAg</td>\n",
       "      <td>axel &amp; yola</td>\n",
       "      <td>2023-07-06 07:38:12+00:00</td>\n",
       "      <td>teknologi baru developnya sempurna mesin bensi...</td>\n",
       "      <td>positif</td>\n",
       "      <td>teknologi baru pengembangannya sempurna mesin ...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "      <td>positif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Ugz15XcYwVifRXEi_ix4AaABAg</td>\n",
       "      <td>Willoughbia elmerii</td>\n",
       "      <td>2023-07-06 05:48:21+00:00</td>\n",
       "      <td>lebih efisien   beli   duit mau korupsi</td>\n",
       "      <td>positif</td>\n",
       "      <td>lebih efisien beli duit mau korupsi</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>UgxyIZFWsg81fFnfZ9V4AaABAg</td>\n",
       "      <td>Boby rico</td>\n",
       "      <td>2023-07-06 03:40:30+00:00</td>\n",
       "      <td>hayo bingung iru punya</td>\n",
       "      <td>positif</td>\n",
       "      <td>hayo bingung itu punya</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>UgyWh6o1hmq0fQPLpVN4AaABAg</td>\n",
       "      <td>Budiman Tedja Saputra</td>\n",
       "      <td>2023-07-06 03:14:39+00:00</td>\n",
       "      <td>banyak bahas efisiensi kurang polusi udara ceo...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>banyak bahas efisiensi kurang polusi udara ceo...</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>UgwhfeyoAqCQWJ59_4J4AaABAg</td>\n",
       "      <td>gameoff</td>\n",
       "      <td>2023-07-06 01:20:11+00:00</td>\n",
       "      <td>mending turu punya malah iklan ampun gusti</td>\n",
       "      <td>positif</td>\n",
       "      <td>mending tidur punya malah iklan ampun gusti</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>UgxuEqopgIM6KhiG5fJ4AaABAg</td>\n",
       "      <td>The Reaction</td>\n",
       "      <td>2023-07-05 23:57:57+00:00</td>\n",
       "      <td>harga jual   mahal banding kendara biasa giman...</td>\n",
       "      <td>positif</td>\n",
       "      <td>harga jual mahal banding kendara biasa gimana ...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>UgypKmila4jVWabelJl4AaABAg</td>\n",
       "      <td>Dcdi Wizaya OFFICIAL</td>\n",
       "      <td>2023-07-05 22:20:00+00:00</td>\n",
       "      <td>sulit hidup promosi monopoli naik harga bensi...</td>\n",
       "      <td>positif</td>\n",
       "      <td>sulit hidup promosi monopoli naik harga bensin...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>UgyoJ-ZI85tr4Nv9ipB4AaABAg</td>\n",
       "      <td>Perspektif</td>\n",
       "      <td>2023-07-05 14:51:55+00:00</td>\n",
       "      <td>stop bbm kelar urus simple</td>\n",
       "      <td>positif</td>\n",
       "      <td>stop bbm kelar urus sederhana</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>UgwX7cskjni__snPest4AaABAg</td>\n",
       "      <td>INFO KINI CHANNEL</td>\n",
       "      <td>2023-07-05 07:53:21+00:00</td>\n",
       "      <td>harga mahal murah konvensional   harga bawah k...</td>\n",
       "      <td>positif</td>\n",
       "      <td>harga mahal murah konvensional harga bawah kon...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>UgwoArUw4TcHprDN1VB4AaABAg</td>\n",
       "      <td>Rangga Gunawan Putra</td>\n",
       "      <td>2023-07-05 06:12:52+00:00</td>\n",
       "      <td>jt bawah bukan harga jangkau orang mikir beli ...</td>\n",
       "      <td>positif</td>\n",
       "      <td>jt bawah bukan harga jangkau orang mikir beli ...</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Ugz0BnS94Xo5FkCo6154AaABAg</td>\n",
       "      <td>didis didis</td>\n",
       "      <td>2023-07-05 02:03:18+00:00</td>\n",
       "      <td>fasilitas buat ngecas bangun dulu baru jual unit</td>\n",
       "      <td>negatif</td>\n",
       "      <td>fasilitas buat mengecas bangun dulu baru jual ...</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>netral</td>\n",
       "      <td>negatif</td>\n",
       "      <td>netral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Ugz73rmR27FUxPnLAlB4AaABAg</td>\n",
       "      <td>fahriadi</td>\n",
       "      <td>2023-07-05 01:24:58+00:00</td>\n",
       "      <td>jalan   banyak     mulus</td>\n",
       "      <td>negatif</td>\n",
       "      <td>jalan banyak mulus</td>\n",
       "      <td>positif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>positif</td>\n",
       "      <td>positif</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Ugw5qs1lYqeVVHxHYtF4AaABAg</td>\n",
       "      <td>Herry Lubis</td>\n",
       "      <td>2023-07-05 01:04:59+00:00</td>\n",
       "      <td>batrenya mahal   rusak</td>\n",
       "      <td>netral</td>\n",
       "      <td>baterainya mahal rusak</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "      <td>negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id_komentar              nama_akun  \\\n",
       "2   UgwqJqu6JMF4EH2CsVV4AaABAg        Fatih Al-Ayyubi   \n",
       "5   Ugx-zVY4ktd7JNUB6xV4AaABAg       Syarif Airlangga   \n",
       "7   Ugyy3luBOOHQspWyBiR4AaABAg          Putut Parwoto   \n",
       "12  UgzZY-ZoTvfiVoOv8Dt4AaABAg                   gema   \n",
       "14  UgwzritHDvD9naYvia54AaABAg           Khoirudin 22   \n",
       "19  UgxGrdA8dD3FNULB4Rd4AaABAg         Lukman Effendi   \n",
       "20  Ugw_osI9is5z9jHD9454AaABAg          Ricky Thunger   \n",
       "23  Ugym8DQyKPAzJntZCeZ4AaABAg    Deddy jagad semesta   \n",
       "25  UgxFMeOytVzzJo6iy5F4AaABAg            TopTrainers   \n",
       "26  UgxcvX9LMnprk35PR3d4AaABAg           parkir anovo   \n",
       "29  Ugz6oZ8eFg8_9QCkfHx4AaABAg              Ary Nawan   \n",
       "30  Ugw4c0eMIKC1XcKtYa14AaABAg                    Zar   \n",
       "31  Ugxnals4WC_p0IuGVsB4AaABAg          agung nugroho   \n",
       "33  UgwAD3ZeI8HU7tJghSZ4AaABAg               RandomWM   \n",
       "35  UgwpTFEpxjsXFSKYaV14AaABAg           yudhi mypras   \n",
       "39  Ugwp4q-M41KTZ4GawjF4AaABAg                   Inta   \n",
       "42  Ugx8_EzV1yvHhp0hK-V4AaABAg            Abner Moses   \n",
       "44  UgwLp2Ai329bzdpRV2x4AaABAg            axel & yola   \n",
       "45  Ugz15XcYwVifRXEi_ix4AaABAg    Willoughbia elmerii   \n",
       "48  UgxyIZFWsg81fFnfZ9V4AaABAg              Boby rico   \n",
       "50  UgyWh6o1hmq0fQPLpVN4AaABAg  Budiman Tedja Saputra   \n",
       "52  UgwhfeyoAqCQWJ59_4J4AaABAg                gameoff   \n",
       "54  UgxuEqopgIM6KhiG5fJ4AaABAg           The Reaction   \n",
       "57  UgypKmila4jVWabelJl4AaABAg   Dcdi Wizaya OFFICIAL   \n",
       "61  UgyoJ-ZI85tr4Nv9ipB4AaABAg             Perspektif   \n",
       "66  UgwX7cskjni__snPest4AaABAg      INFO KINI CHANNEL   \n",
       "68  UgwoArUw4TcHprDN1VB4AaABAg   Rangga Gunawan Putra   \n",
       "73  Ugz0BnS94Xo5FkCo6154AaABAg            didis didis   \n",
       "74  Ugz73rmR27FUxPnLAlB4AaABAg               fahriadi   \n",
       "75  Ugw5qs1lYqeVVHxHYtF4AaABAg            Herry Lubis   \n",
       "\n",
       "                      tanggal  \\\n",
       "2   2023-08-04 10:17:57+00:00   \n",
       "5   2023-08-04 06:58:17+00:00   \n",
       "7   2023-08-04 01:04:18+00:00   \n",
       "12  2023-08-03 04:23:25+00:00   \n",
       "14  2023-07-29 11:08:07+00:00   \n",
       "19  2023-07-28 10:23:53+00:00   \n",
       "20  2023-07-27 09:20:51+00:00   \n",
       "23  2023-07-20 07:40:34+00:00   \n",
       "25  2023-07-16 06:30:59+00:00   \n",
       "26  2023-07-13 05:17:26+00:00   \n",
       "29  2023-07-12 06:29:27+00:00   \n",
       "30  2023-07-11 15:18:15+00:00   \n",
       "31  2023-07-11 14:14:22+00:00   \n",
       "33  2023-07-10 13:16:11+00:00   \n",
       "35  2023-07-09 22:51:21+00:00   \n",
       "39  2023-07-07 03:56:45+00:00   \n",
       "42  2023-07-06 11:38:46+00:00   \n",
       "44  2023-07-06 07:38:12+00:00   \n",
       "45  2023-07-06 05:48:21+00:00   \n",
       "48  2023-07-06 03:40:30+00:00   \n",
       "50  2023-07-06 03:14:39+00:00   \n",
       "52  2023-07-06 01:20:11+00:00   \n",
       "54  2023-07-05 23:57:57+00:00   \n",
       "57  2023-07-05 22:20:00+00:00   \n",
       "61  2023-07-05 14:51:55+00:00   \n",
       "66  2023-07-05 07:53:21+00:00   \n",
       "68  2023-07-05 06:12:52+00:00   \n",
       "73  2023-07-05 02:03:18+00:00   \n",
       "74  2023-07-05 01:24:58+00:00   \n",
       "75  2023-07-05 01:04:59+00:00   \n",
       "\n",
       "                                        text_cleaning sentimen  \\\n",
       "2   baik kualitas kembang dulu baik kualitas motor...  positif   \n",
       "5   harga motor mahal masa harga mirip motor beat ...  positif   \n",
       "7   proses kenal produk baru butuh waktu ganti ken...  negatif   \n",
       "12  kampung   sekarang banyak banget bocil sama ce...  positif   \n",
       "14                                harga terlalu mahal  positif   \n",
       "19  bapak luhut panjaitan yth baik pakai dulu dina...  positif   \n",
       "20            subsidi motor   jalan umum tolak system  negatif   \n",
       "23                          tambah bikin rakyat susah   netral   \n",
       "25             jual ev naik jual genset bakal naik ni  positif   \n",
       "26    contohin dulu semua menteri dpr mpr guna listri  negatif   \n",
       "29                                  harga sperpartnya  positif   \n",
       "30  ev msih nyaman dsni   dipaksain paling cman ja...  positif   \n",
       "31  nungguin ganti lithium solid state battery ril...  positif   \n",
       "33  suat   baru kata   jaman dlu kala efisien ting...  positif   \n",
       "35    masyarakat ingin harga jual batre   murah ba...  positif   \n",
       "39    coba cek jabat staf ngomong soal sehariya pakai  negatif   \n",
       "42               lama bensin   laku harga lebih mahal  positif   \n",
       "44  teknologi baru developnya sempurna mesin bensi...  positif   \n",
       "45          lebih efisien   beli   duit mau korupsi    positif   \n",
       "48                             hayo bingung iru punya  positif   \n",
       "50  banyak bahas efisiensi kurang polusi udara ceo...  negatif   \n",
       "52         mending turu punya malah iklan ampun gusti  positif   \n",
       "54  harga jual   mahal banding kendara biasa giman...  positif   \n",
       "57   sulit hidup promosi monopoli naik harga bensi...  positif   \n",
       "61                         stop bbm kelar urus simple  positif   \n",
       "66  harga mahal murah konvensional   harga bawah k...  positif   \n",
       "68  jt bawah bukan harga jangkau orang mikir beli ...  positif   \n",
       "73   fasilitas buat ngecas bangun dulu baru jual unit  negatif   \n",
       "74                           jalan   banyak     mulus  negatif   \n",
       "75                             batrenya mahal   rusak   netral   \n",
       "\n",
       "                                          mapped_text sentimen_RoBERTa  \\\n",
       "2   baik kualitas kembang dulu baik kualitas motor...          positif   \n",
       "5   harga motor mahal masa harga mirip motor beat ...          negatif   \n",
       "7   proses kenal produk baru butuh waktu ganti ken...           netral   \n",
       "12  kampung sekarang banyak banget bocil sama cewe...           netral   \n",
       "14                                harga terlalu mahal          negatif   \n",
       "19  bapak luhut panjaitan yth baik pakai dulu dina...          negatif   \n",
       "20              subsidi motor jalan umum tolak sistem           netral   \n",
       "23                          tambah bikin rakyat susah          negatif   \n",
       "25             jual ev naik jual genset bakal naik ni           netral   \n",
       "26  contohkan dulu semua menteri dpr mpr guna listrik           netral   \n",
       "29                               harga suku cadangnya           netral   \n",
       "30  ev masih nyaman disini dipaksakan paling cuman...          positif   \n",
       "31  menunggu ganti lithium solid negara baterai ri...           netral   \n",
       "33  suat baru kata jaman dulu kala efisien tinggal...          negatif   \n",
       "35  masyarakat ingin harga jual baterai murah baru...          positif   \n",
       "39  coba cek jabat staf ngomong soal sehari-hari p...           netral   \n",
       "42                 lama bensin laku harga lebih mahal          negatif   \n",
       "44  teknologi baru pengembangannya sempurna mesin ...          negatif   \n",
       "45                lebih efisien beli duit mau korupsi          negatif   \n",
       "48                             hayo bingung itu punya          negatif   \n",
       "50  banyak bahas efisiensi kurang polusi udara ceo...           netral   \n",
       "52        mending tidur punya malah iklan ampun gusti          negatif   \n",
       "54  harga jual mahal banding kendara biasa gimana ...          negatif   \n",
       "57  sulit hidup promosi monopoli naik harga bensin...          negatif   \n",
       "61                      stop bbm kelar urus sederhana          negatif   \n",
       "66  harga mahal murah konvensional harga bawah kon...          negatif   \n",
       "68  jt bawah bukan harga jangkau orang mikir beli ...          negatif   \n",
       "73  fasilitas buat mengecas bangun dulu baru jual ...           netral   \n",
       "74                                 jalan banyak mulus          positif   \n",
       "75                             baterainya mahal rusak          negatif   \n",
       "\n",
       "   sentimen_DistilBERT sentimen_BERT sentimen_INDOBERTweet  \\\n",
       "2               netral       negatif               negatif   \n",
       "5               netral       negatif               negatif   \n",
       "7               netral        netral               negatif   \n",
       "12              netral       negatif               negatif   \n",
       "14             negatif       negatif               negatif   \n",
       "19              netral        netral               negatif   \n",
       "20              netral        netral                netral   \n",
       "23             negatif       negatif               negatif   \n",
       "25              netral        netral               negatif   \n",
       "26              netral        netral                netral   \n",
       "29             positif        netral                netral   \n",
       "30              netral       negatif               negatif   \n",
       "31              netral        netral                netral   \n",
       "33             negatif       negatif               negatif   \n",
       "35              netral        netral               negatif   \n",
       "39              netral        netral                netral   \n",
       "42             negatif       negatif               negatif   \n",
       "44              netral       positif               negatif   \n",
       "45             negatif       negatif               negatif   \n",
       "48             negatif       negatif                netral   \n",
       "50              netral        netral               negatif   \n",
       "52             negatif       negatif               negatif   \n",
       "54             negatif       negatif               negatif   \n",
       "57             negatif       negatif               negatif   \n",
       "61              netral       negatif               negatif   \n",
       "66             negatif       negatif               negatif   \n",
       "68             negatif       negatif               negatif   \n",
       "73              netral        netral               negatif   \n",
       "74             negatif       positif               positif   \n",
       "75             negatif       negatif               negatif   \n",
       "\n",
       "   final_ensemble_sentiment  \n",
       "2                   negatif  \n",
       "5                   negatif  \n",
       "7                    netral  \n",
       "12                  negatif  \n",
       "14                  negatif  \n",
       "19                  negatif  \n",
       "20                   netral  \n",
       "23                  negatif  \n",
       "25                   netral  \n",
       "26                   netral  \n",
       "29                   netral  \n",
       "30                  negatif  \n",
       "31                   netral  \n",
       "33                  negatif  \n",
       "35                   netral  \n",
       "39                   netral  \n",
       "42                  negatif  \n",
       "44                  negatif  \n",
       "45                  negatif  \n",
       "48                  negatif  \n",
       "50                   netral  \n",
       "52                  negatif  \n",
       "54                  negatif  \n",
       "57                  negatif  \n",
       "61                  negatif  \n",
       "66                  negatif  \n",
       "68                  negatif  \n",
       "73                   netral  \n",
       "74                  positif  \n",
       "75                  negatif  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.final_ensemble_sentiment != df.sentimen].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1362\n",
      "Test data: 152\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "# show len\n",
    "print('Train data:', len(X_train))\n",
    "print('Test data:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_weights = list(class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                         classes=np.unique(df['final_ensemble_sentiment']),\n",
    "                                                         y=df['final_ensemble_sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_ensemble_sentiment\n",
       "negatif    1150\n",
       "netral      235\n",
       "positif     129\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['final_ensemble_sentiment'].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vectorizer': 'tfidf',  # Choose 'tfidf' or 'count'\n",
    "    'vectorizer_params': {'max_features': 5000},  # Additional parameters for vectorizer\n",
    "    'model_params': {},  # Additional parameters for models\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "model_dict = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='multi:softmax', num_class=3, **config['model_params'], class_weights=classes_weights),\n",
    "    'randomforest': RandomForestClassifier(**config['model_params']),\n",
    "    'svm': SVC(**config['model_params']),\n",
    "    'logreg': LogisticRegression(**config['model_params']),\n",
    "    'lightgbm': lgb.LGBMClassifier(**config['model_params'], class_weights=classes_weights),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\pipeline.py\", line 476, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1284, in fit\n    super().fit(\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 955, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\engine.py\", line 282, in train\n    booster = Booster(params=params, train_set=train_set)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 3637, in __init__\n    train_set.construct()\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 2576, in construct\n    self._lazy_init(\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 2144, in _lazy_init\n    params_str = _param_dict_to_str(params)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 537, in _param_dict_to_str\n    raise TypeError(f\"Unknown type of parameter:{key}, got:{type(val).__name__}\")\nTypeError: Unknown type of parameter:class_weights, got:dict\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     13\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m, vectorizer),\n\u001b[0;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, model),\n\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Perform cross-validation for different metrics on the training set\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1_weighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m mean_f1_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(f1_scores)\n\u001b[0;32m     21\u001b[0m accuracy_scores \u001b[38;5;241m=\u001b[39m cross_val_score(pipeline, X_train, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[1;32m--> 443\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\sklearn\\pipeline.py\", line 476, in fit\n    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 1284, in fit\n    super().fit(\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\sklearn.py\", line 955, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\engine.py\", line 282, in train\n    booster = Booster(params=params, train_set=train_set)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 3637, in __init__\n    train_set.construct()\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 2576, in construct\n    self._lazy_init(\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 2144, in _lazy_init\n    params_str = _param_dict_to_str(params)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\user\\anaconda3\\envs\\gnn\\Lib\\site-packages\\lightgbm\\basic.py\", line 537, in _param_dict_to_str\n    raise TypeError(f\"Unknown type of parameter:{key}, got:{type(val).__name__}\")\nTypeError: Unknown type of parameter:class_weights, got:dict\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(**config['vectorizer_params'])\n",
    "\n",
    "# List to store results and a dictionary to store predictions\n",
    "results = []\n",
    "predictions_dict = {}\n",
    "\n",
    "# Iterate through the models and evaluate them\n",
    "for model_name, model in model_dict.items():\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('model', model),\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation for different metrics on the training set\n",
    "    f1_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    accuracy_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_accuracy_score = np.mean(accuracy_scores)\n",
    "    \n",
    "    precision_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='precision_weighted')\n",
    "    mean_precision_score = np.mean(precision_scores)\n",
    "    \n",
    "    recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall_weighted')\n",
    "    mean_recall_score = np.mean(recall_scores)\n",
    "    \n",
    "    # Fit the model on the full training set\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Store predictions in the dictionary\n",
    "    predictions_dict[model_name] = y_pred\n",
    "    \n",
    "    # Compute metrics on the test set\n",
    "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Append the results (train and test metrics)\n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'train_f1': mean_f1_score,\n",
    "        'train_accuracy': mean_accuracy_score,\n",
    "        'train_precision': mean_precision_score,\n",
    "        'train_recall': mean_recall_score,\n",
    "        'test_f1': test_f1,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall\n",
    "    })\n",
    "\n",
    "# Print the results in a table format\n",
    "print(f\"{'Model':<15} {'Train F1':<10} {'Train Acc':<10} {'Train Prec':<10} {'Train Recall':<10} {'Test F1':<10} {'Test Acc':<10} {'Test Prec':<10} {'Test Recall':<10}\")\n",
    "for result in results:\n",
    "    print(f\"{result['model']:<15} {result['train_f1']:.4f}    {result['train_accuracy']:.4f}   {result['train_precision']:.4f}    {result['train_recall']:.4f}    \"\n",
    "          f\"{result['test_f1']:.4f}    {result['test_accuracy']:.4f}   {result['test_precision']:.4f}    {result['test_recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# make array serializeable\n",
    "# for key, value in predictions_dict.items():\n",
    "#     predictions_dict[key] = value.tolist()\n",
    "\n",
    "# Save the predictions to a JSON file\n",
    "with open('predictions_ml_ensemble.json', 'w') as f:\n",
    "    json.dump(predictions_dict, f)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results_ml_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1210\n",
      "Val data: 152\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=3)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "X_train_bert, X_val_bert, y_train_bert, y_val_bert = train_test_split(X_train, y_train, test_size=0.111, random_state=42)\n",
    "\n",
    "print('Train data:', len(X_train_bert))\n",
    "print('Val data:', len(X_val_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenize_function(list(X_train_bert))\n",
    "X_val_tokenized = tokenize_function(list(X_val_bert))\n",
    "\n",
    "# Convert labels to lists of integers\n",
    "y_train_list = y_train_bert.tolist()\n",
    "y_val_list = y_val_bert.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': [ids.tolist() for ids in X_train_tokenized['input_ids']],  # Convert tensors to lists\n",
    "    'attention_mask': [mask.tolist() for mask in X_train_tokenized['attention_mask']],  # Convert tensors to lists\n",
    "    'labels': y_train_list\n",
    "})\n",
    "\n",
    "eval_dataset = Dataset.from_dict({\n",
    "    'input_ids': [ids.tolist() for ids in X_val_tokenized['input_ids']],  # Convert tensors to lists\n",
    "    'attention_mask': [mask.tolist() for mask in X_val_tokenized['attention_mask']],  # Convert tensors to lists\n",
    "    'labels': y_val_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e1cfa2378f42a98e98bc1634a699c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8617, 'grad_norm': 47.028038024902344, 'learning_rate': 4e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c21c96cceae409e9ac86039e393a5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8264140486717224, 'eval_accuracy': 0.631578947368421, 'eval_f1': 0.6106526478798622, 'eval_precision': 0.6313230994152047, 'eval_recall': 0.631578947368421, 'eval_runtime': 1.2545, 'eval_samples_per_second': 121.163, 'eval_steps_per_second': 15.145, 'epoch': 1.0}\n",
      "{'loss': 0.5498, 'grad_norm': 69.61203002929688, 'learning_rate': 3e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4e403b6bc24e12b7e659b8422486e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7540877461433411, 'eval_accuracy': 0.7368421052631579, 'eval_f1': 0.7174865823537059, 'eval_precision': 0.7186380872129767, 'eval_recall': 0.7368421052631579, 'eval_runtime': 1.2127, 'eval_samples_per_second': 125.338, 'eval_steps_per_second': 15.667, 'epoch': 2.0}\n",
      "{'loss': 0.2641, 'grad_norm': 0.15904687345027924, 'learning_rate': 2e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5e874ee4f4498eaf5b9df99c25804e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2714046239852905, 'eval_accuracy': 0.7105263157894737, 'eval_f1': 0.706668190326934, 'eval_precision': 0.7031774916013438, 'eval_recall': 0.7105263157894737, 'eval_runtime': 1.2572, 'eval_samples_per_second': 120.903, 'eval_steps_per_second': 15.113, 'epoch': 3.0}\n",
      "{'loss': 0.1325, 'grad_norm': 0.01548162940889597, 'learning_rate': 1e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae6e670079d46ec8de42042a4139b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5487794876098633, 'eval_accuracy': 0.7368421052631579, 'eval_f1': 0.7206778823064102, 'eval_precision': 0.7206697298802561, 'eval_recall': 0.7368421052631579, 'eval_runtime': 1.2231, 'eval_samples_per_second': 124.276, 'eval_steps_per_second': 15.535, 'epoch': 4.0}\n",
      "{'loss': 0.0313, 'grad_norm': 0.011891782283782959, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02db8c6d5c664d5bae53c94731d160d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4958992004394531, 'eval_accuracy': 0.75, 'eval_f1': 0.7533178524894382, 'eval_precision': 0.759915080253826, 'eval_recall': 0.75, 'eval_runtime': 1.3121, 'eval_samples_per_second': 115.843, 'eval_steps_per_second': 14.48, 'epoch': 5.0}\n",
      "{'train_runtime': 193.6529, 'train_samples_per_second': 31.241, 'train_steps_per_second': 3.925, 'train_loss': 0.3678650699163738, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=760, training_loss=0.3678650699163738, metrics={'train_runtime': 193.6529, 'train_samples_per_second': 31.241, 'train_steps_per_second': 3.925, 'total_flos': 397959044313600.0, 'train_loss': 0.3678650699163738, 'epoch': 5.0})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"epoch\",  # Use 'eval_strategy' in future versions\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Use Huggingface's Trainer API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 0.8617,\n",
       "  'grad_norm': 47.028038024902344,\n",
       "  'learning_rate': 4e-05,\n",
       "  'epoch': 1.0,\n",
       "  'step': 152},\n",
       " {'eval_loss': 0.8264140486717224,\n",
       "  'eval_accuracy': 0.631578947368421,\n",
       "  'eval_f1': 0.6106526478798622,\n",
       "  'eval_precision': 0.6313230994152047,\n",
       "  'eval_recall': 0.631578947368421,\n",
       "  'eval_runtime': 1.2545,\n",
       "  'eval_samples_per_second': 121.163,\n",
       "  'eval_steps_per_second': 15.145,\n",
       "  'epoch': 1.0,\n",
       "  'step': 152},\n",
       " {'loss': 0.5498,\n",
       "  'grad_norm': 69.61203002929688,\n",
       "  'learning_rate': 3e-05,\n",
       "  'epoch': 2.0,\n",
       "  'step': 304},\n",
       " {'eval_loss': 0.7540877461433411,\n",
       "  'eval_accuracy': 0.7368421052631579,\n",
       "  'eval_f1': 0.7174865823537059,\n",
       "  'eval_precision': 0.7186380872129767,\n",
       "  'eval_recall': 0.7368421052631579,\n",
       "  'eval_runtime': 1.2127,\n",
       "  'eval_samples_per_second': 125.338,\n",
       "  'eval_steps_per_second': 15.667,\n",
       "  'epoch': 2.0,\n",
       "  'step': 304},\n",
       " {'loss': 0.2641,\n",
       "  'grad_norm': 0.15904687345027924,\n",
       "  'learning_rate': 2e-05,\n",
       "  'epoch': 3.0,\n",
       "  'step': 456},\n",
       " {'eval_loss': 1.2714046239852905,\n",
       "  'eval_accuracy': 0.7105263157894737,\n",
       "  'eval_f1': 0.706668190326934,\n",
       "  'eval_precision': 0.7031774916013438,\n",
       "  'eval_recall': 0.7105263157894737,\n",
       "  'eval_runtime': 1.2572,\n",
       "  'eval_samples_per_second': 120.903,\n",
       "  'eval_steps_per_second': 15.113,\n",
       "  'epoch': 3.0,\n",
       "  'step': 456},\n",
       " {'loss': 0.1325,\n",
       "  'grad_norm': 0.01548162940889597,\n",
       "  'learning_rate': 1e-05,\n",
       "  'epoch': 4.0,\n",
       "  'step': 608},\n",
       " {'eval_loss': 1.5487794876098633,\n",
       "  'eval_accuracy': 0.7368421052631579,\n",
       "  'eval_f1': 0.7206778823064102,\n",
       "  'eval_precision': 0.7206697298802561,\n",
       "  'eval_recall': 0.7368421052631579,\n",
       "  'eval_runtime': 1.2231,\n",
       "  'eval_samples_per_second': 124.276,\n",
       "  'eval_steps_per_second': 15.535,\n",
       "  'epoch': 4.0,\n",
       "  'step': 608},\n",
       " {'loss': 0.0313,\n",
       "  'grad_norm': 0.011891782283782959,\n",
       "  'learning_rate': 0.0,\n",
       "  'epoch': 5.0,\n",
       "  'step': 760},\n",
       " {'eval_loss': 1.4958992004394531,\n",
       "  'eval_accuracy': 0.75,\n",
       "  'eval_f1': 0.7533178524894382,\n",
       "  'eval_precision': 0.759915080253826,\n",
       "  'eval_recall': 0.75,\n",
       "  'eval_runtime': 1.3121,\n",
       "  'eval_samples_per_second': 115.843,\n",
       "  'eval_steps_per_second': 14.48,\n",
       "  'epoch': 5.0,\n",
       "  'step': 760},\n",
       " {'train_runtime': 193.6529,\n",
       "  'train_samples_per_second': 31.241,\n",
       "  'train_steps_per_second': 3.925,\n",
       "  'total_flos': 397959044313600.0,\n",
       "  'train_loss': 0.3678650699163738,\n",
       "  'epoch': 5.0,\n",
       "  'step': 760}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6091c6048948d1afd2b056552f790e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83        95\n",
      "           1       0.39      0.39      0.39        18\n",
      "           2       0.65      0.67      0.66        39\n",
      "\n",
      "    accuracy                           0.73       152\n",
      "   macro avg       0.62      0.63      0.62       152\n",
      "weighted avg       0.73      0.73      0.73       152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': [ids.tolist() for ids in tokenize_function(list(X_test))['input_ids']],  # Convert tensors to lists\n",
    "    'attention_mask': [mask.tolist() for mask in tokenize_function(list(X_test))['attention_mask']],  # Convert tensors to lists\n",
    "    'labels': y_test.tolist()\n",
    "})\n",
    "y_test_list = y_test.tolist()\n",
    "\n",
    "predictions = trainer.predict(test_dataset) \n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "print(classification_report(y_test_list, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the NLTK tokenizer if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load sentiment lexicons\n",
    "def load_lexicon(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "    return dict(zip(df['word'], df['weight']))\n",
    "\n",
    "positive_lexicon = load_lexicon('positive.tsv')\n",
    "negative_lexicon = load_lexicon('negative.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(text, pos_lexicon, neg_lexicon):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    score = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in pos_lexicon:\n",
    "            score += pos_lexicon[token]\n",
    "        elif token in neg_lexicon:\n",
    "            score += neg_lexicon[token]\n",
    "    \n",
    "    return score\n",
    "\n",
    "def classify_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'Positive'\n",
    "    elif score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Score: 2\n",
      "Sentiment: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# nltk.download()\n",
    "nltk.download('punkt_tab')\n",
    "text = \"hai merekam detail isak\"\n",
    "sentiment_score = get_sentiment_score(text, positive_lexicon, negative_lexicon)\n",
    "sentiment = classify_sentiment(sentiment_score)\n",
    "\n",
    "print(f\"Sentiment Score: {sentiment_score}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
